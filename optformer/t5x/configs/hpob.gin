# Copyright 2022 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Register necessary SeqIO Tasks/Mixtures.
#
# Configuration for models trained on the HPO-B dataset with all HPO algorithms.
# Quantization Q = 1000
# Random objective range augmentation.
#
# inputs length = 512
# targets lengths = 1024
# max_trials = 200

from __gin__ import dynamic_registration

from optformer.data import converters
from optformer.data import tasks as t5_tasks


include 'optformer/t5x/configs/models/t5_1_1_base.gin'


TARGET_LENGTHS = 1024
TASK_FEATURE_LENGTHS = {
    'inputs': 512,  # Maximum input seq after tokenizatio is 312.
    'targets': %TARGET_LENGTHS,
    'target_inputs': %TARGET_LENGTHS,
    'targets_types': %TARGET_LENGTHS,
    'targets_masks': %TARGET_LENGTHS,
}
DROPOUT_RATE = 0.0

# Make sure the model's vocabulary object and size match the task's.
VOCABULARY = @t5_tasks.get_vocabulary()
NUM_EMBEDDINGS = 33152  # vocab size rounded to a multiple of 128 for TPU efficiency
t5_tasks.get_vocabulary:
  max_integer_tokens = 1000
  expected_vocab_size = 33100  # max_integer_tokens + 1000 <= %NUM_EMBEDDINGS
VOCAB_INDEX_FROM = 32100  # First token index of integers.

# Initial number of tokens in the target string before the first trial starts.
NUM_INITIAL_TOKENS = 1

t5_tasks.add_tasks:
  vocabulary = %VOCABULARY
  masked_types = ['separator']  # Target masks in defining the training loss.
  num_initial_tokens = %NUM_INITIAL_TOKENS
  add_eos_in_targets = False

# Config for study converter.
STUDY_CONVERTER = @converters.OptFormerConverter()
converters.OptFormerConverter:
  rand_objective_scale_range = (0.3, 1.0)
  max_trials = 200
  num_initial_tokens = %NUM_INITIAL_TOKENS
  minimum_config = False  # Always use all metadata in the HPO-B transfer setting.
  minimum_config_per_study = False
