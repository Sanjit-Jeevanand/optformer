# Copyright 2022 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Vizier T5 model specific overrides.
#
# Quantization Q = 1000

from __gin__ import dynamic_registration

from optformer.data import tasks as t5_tasks
from optformer.t5x import decoding as vizier_decoding
from optformer.t5x import models as vizier_models
from t5x.examples.t5 import network
from t5x import partitioning
from t5x import utils

include 't5x/examples/t5/t5_1_1/base.gin'

MODEL = @vizier_models.VizierEncoderDecoderModel()
vizier_models.VizierEncoderDecoderModel:
  module = @network.Transformer()
  input_vocabulary = %VOCABULARY
  output_vocabulary = %VOCABULARY
  optimizer_def = %OPTIMIZER
  decode_fn = @vizier_decoding.temperature_sample
  z_loss = %Z_LOSS
  label_smoothing = %LABEL_SMOOTHING
  loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR

network.T5Config:
  vocab_size = %NUM_EMBEDDINGS

NUM_PARTITIONS = 1  # Exclusive with MODEL_PARALLEL_SUBMESH
MODEL_PARALLEL_SUBMESH = None
partitioning.PjitPartitioner:
  num_partitions = %NUM_PARTITIONS
  model_parallel_submesh = %MODEL_PARALLEL_SUBMESH

# Sample
TOPK = 0  # Do not truncate the predictive distribution.
TEMPERATURE = 1.0
MAX_DECODE_STEPS = None
vizier_decoding.temperature_sample:
  temperature = %TEMPERATURE
  topk = %TOPK
  max_decode_len = %MAX_DECODE_STEPS
vizier_models.VizierEncoderDecoderModel.decode_fn = @vizier_decoding.temperature_sample

NUM_DECODES = 1
RETURN_ALL_DECODES = True
vizier_models.VizierEncoderDecoderModel.predict_batch_with_aux:
  num_decodes = %NUM_DECODES
  return_all_decodes = %RETURN_ALL_DECODES

CHECKPOINT_PATH = []
utils.RestoreCheckpointConfig:
  path = %CHECKPOINT_PATH
  mode = 'specific'
  dtype = 'float32'

# Make sure the model's vocabulary object and size match the task's.
VOCABULARY = @t5_tasks.get_vocabulary()
NUM_EMBEDDINGS = 33152  # vocab size rounded to a multiple of 128 for TPU efficiency
t5_tasks.get_vocabulary:
  max_integer_tokens = 1000
  expected_vocab_size = 33100  # max_integer_tokens + 1000 <= %NUM_EMBEDDINGS
VOCAB_INDEX_FROM = 32100  # First token index of integers.

# Initial number of tokens in the target string before the first trial starts.
NUM_INITIAL_TOKENS = 1

t5_tasks.add_tasks:
  vocabulary = %VOCABULARY
  masked_types = ['separator']  # Target masks in defining the training loss.
  num_initial_tokens = %NUM_INITIAL_TOKENS
  add_eos_in_targets = False
