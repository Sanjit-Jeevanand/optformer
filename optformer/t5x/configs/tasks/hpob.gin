# Copyright 2022 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Register necessary SeqIO Tasks/Mixtures.
#
# Configuration for models trained on the HPO-B dataset with all HPO algorithms.
# Random objective range augmentation.
#
# inputs length = 512
# targets lengths = 1024
# max_trials = 200

from __gin__ import dynamic_registration

from optformer.data import converters
from t5x import utils

include 't5x/examples/t5/t5_1_1/base.gin'
include 'optformer/t5x/configs/models/t5_1_1_overrides.gin'

USE_CACHED_TASKS = True
MIXTURE_OR_TASK_NAME = 'cached_hpob_gp_train_repeat_30'
train_eval/utils.DatasetConfig:
  mixture_or_task_name = 'cached_hpob_gp_eval'

TRAIN_STEPS = 1_000_000
LEARNING_RATE = 0.3
DROPOUT_RATE = 0.3

TARGET_LENGTHS = 1024
TASK_FEATURE_LENGTHS = {
    'inputs': 512,  # Maximum input seq after tokenizatio is 312.
    'targets': %TARGET_LENGTHS,
    'target_inputs': %TARGET_LENGTHS,
    'targets_types': %TARGET_LENGTHS,
    'targets_masks': %TARGET_LENGTHS,
}

# Config for study converter.
STUDY_CONVERTER = @converters.OptFormerConverter()
converters.OptFormerConverter:
  rand_objective_scale_range = (0.3, 1.0)
  max_trials = 200
  num_initial_tokens = %NUM_INITIAL_TOKENS
  minimum_config = False  # Always use all metadata in the HPO-B transfer setting.
  minimum_config_per_study = False
