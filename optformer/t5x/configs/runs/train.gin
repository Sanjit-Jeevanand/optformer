# Copyright 2022 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Defaults for pretraining with train.py.
#
# You must also include a binding for MODEL.
#
# Required to be set:
#
# - MIXTURE_OR_TASK_NAME
# - TASK_FEATURE_LENGTHS
# - TRAIN_STEPS
# - MODEL_DIR: # automatically set when using xm_launch
#
# Commonly overridden options:
#
# - train/DatasetConfig.batch_size
# - train_eval/DatasetConfig.batch_size
# - PjitPartitioner.num_partitions
# - Trainer.num_microbatches
# - DROPOUT_RATE
# - LEARNING_RATE
# - BATCH_SIZE

from __gin__ import dynamic_registration

include 't5x/configs/runs/pretrain.gin'

import __main__ as train_script
from t5x import utils

BATCH_SIZE = 256
train/utils.DatasetConfig.batch_size = %BATCH_SIZE
train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE
infer_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE

# Disable packing to avoid eval dataset being changed due to change of input
# string lengths.
train_eval/utils.DatasetConfig.pack = False

EVAL_STEPS = 20

# Can be overridden with `train.*`.`
train_script.train:
  eval_steps = %EVAL_STEPS

utils.SaveCheckpointConfig:
  period = 10000

FACTORS = 'constant * rsqrt_decay'
LEARNING_RATE = 1.0
WARMUP_STEPS = 10_000
DECAY_FACTOR = 0.5
STEPS_PER_DECAY = 20000
STEPS_PER_CYCLE = 100000
utils.create_learning_rate_scheduler:
  factors = %FACTORS
  base_learning_rate = %LEARNING_RATE
  warmup_steps = %WARMUP_STEPS
  decay_factor = %DECAY_FACTOR
  steps_per_decay = %STEPS_PER_DECAY
  steps_per_cycle = %STEPS_PER_CYCLE
