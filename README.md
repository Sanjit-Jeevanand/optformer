# OptFormer: Language Modeling for Optimization
**NOTE:** The codebase in the `main` branch is currently being significantly refactored.

## NeurIPS 2022
**IMPORTANT:** Legacy code for the paper [Towards Learning Universal Hyperparameter Optimizers with Transformers (NeurIPS 2022)](https://arxiv.org/abs/2205.13320) can be found in the [`neurips22` branch](https://github.com/google-research/optformer/tree/neurips22).

Numerical results used in the paper (for e.g. plotting and comparisons) can be found in the [results folder](https://github.com/google-research/optformer/tree/main/optformer/results).

# Citation
If you found this codebase useful, please consider citing our paper. Thanks!

```
@inproceedings{optformer,
  author    = {Yutian Chen and
               Xingyou Song and
               Chansoo Lee and
               Zi Wang and
               Qiuyi Zhang and
               David Dohan and
               Kazuya Kawakami and
               Greg Kochanski and
               Arnaud Doucet and
               Marc'Aurelio Ranzato and
               Sagi Perel and
               Nando de Freitas},
  title     = {Towards Learning Universal Hyperparameter Optimizers with Transformers},
  booktitle = {Neural Information Processing Systems (NeurIPS) 2022},
  year      = {2022}
}
```

**Disclaimer:** This is not an officially supported Google product.
